WARNING: Logging before flag parsing goes to stderr.
E0903 11:06:07.431442 140674526709568 print_helper.py:68] [31m{'delete': False, 'mode': '', 'dataset': 'numpy', 'num_tfrecord_files': 5}[0m
  0%|          | 0/5 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 7977.00it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 5319.35it/s]
I0903 11:06:07.820079 140674526709568 estimator.py:209] Using config: {'_model_dir': '/opt/tf_issue_32052/data/fwd_nnet', '_tf_random_seed': None, '_save_summary_steps': 78.125, '_save_checkpoints_steps': 234.375, '_save_checkpoints_secs': None, '_session_config': gpu_options {
  allow_growth: true
}
allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 78.125, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0c03a53c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
W0903 11:06:07.821169 140674526709568 model_fn.py:630] Estimator's model_fn (<simple_ffwd_net.NNet object at 0x7ff0c0454710>) includes params argument, but params are not passed to Estimator.
I0903 11:06:07.821994 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:06:07.822295 140674526709568 print_helper.py:77] [93mMemory used is 303.6796875[0m
I0903 11:06:07.822560 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
  0%|          | 0/5 [00:00<?, ?it/s]E0903 11:06:09.279133 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:06:09.279715 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:06:09.279984 140674526709568 print_helper.py:77] [93mMemory used is 305.515625[0m
I0903 11:06:09.280238 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
E0903 11:06:09.280733 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training[0m
Writing to /opt/tf_issue_32052/data/train_data/0.tfrecords
Found : /opt/tf_issue_32052/data/train_data/0.tfrecords
Writing to /opt/tf_issue_32052/data/train_data/1.tfrecords
Found : /opt/tf_issue_32052/data/train_data/1.tfrecords
Writing to /opt/tf_issue_32052/data/train_data/2.tfrecords
Found : /opt/tf_issue_32052/data/train_data/2.tfrecords
Writing to /opt/tf_issue_32052/data/train_data/3.tfrecords
Found : /opt/tf_issue_32052/data/train_data/3.tfrecords
Writing to /opt/tf_issue_32052/data/train_data/4.tfrecords
Found : /opt/tf_issue_32052/data/train_data/4.tfrecords
Writing to /opt/tf_issue_32052/data/val_data/0.tfrecords
Found : /opt/tf_issue_32052/data/val_data/0.tfrecords
Writing to /opt/tf_issue_32052/data/val_data/1.tfrecords
Found : /opt/tf_issue_32052/data/val_data/1.tfrecords
Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   142    303.4 MiB    303.4 MiB   @profile
   143                             def gen_data(number_files,
   144                                          IS_EAST_IMAGE_TEST,
   145                                          TRAIN_DATA,
   146                                          VAL_DATA,
   147                                          NUM_SAMPLES_PER_FILE,
   148                                          NUM_FEATURES=None):
   149    303.4 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   150                                     generate_image_tf_records(number_files=number_files,
   151                                                               out_dir=TRAIN_DATA,
   152                                                               NUM_SAMPLES_PER_FILE=NUM_SAMPLES_PER_FILE)
   153                                     generate_image_tf_records(number_files=2, #TODO Fixed?
   154                                                               out_dir=VAL_DATA,
   155                                                               NUM_SAMPLES_PER_FILE=NUM_SAMPLES_PER_FILE)
   156                                 else:
   157    303.4 MiB      0.0 MiB           generate_numpy_tf_records(number_files=number_files,
   158    303.4 MiB      0.0 MiB                                     out_dir=TRAIN_DATA,
   159    303.4 MiB      0.0 MiB                                     NUM_FEATURES=NUM_FEATURES,
   160    303.7 MiB      0.3 MiB                                     NUM_SAMPLES_PER_FILE=NUM_SAMPLES_PER_FILE)
   161    303.7 MiB      0.0 MiB           generate_numpy_tf_records(number_files=2, #TODO fixed?
   162    303.7 MiB      0.0 MiB                                     out_dir=VAL_DATA,
   163    303.7 MiB      0.0 MiB                                     NUM_FEATURES=NUM_FEATURES,
   164    303.7 MiB      0.0 MiB                                     NUM_SAMPLES_PER_FILE=NUM_SAMPLES_PER_FILE)


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
     6    303.7 MiB    303.7 MiB       @profile
     7                                 def __init__(self):
     8    303.7 MiB      0.0 MiB           pass


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    41    303.7 MiB    303.7 MiB   @profile
    42                             def _init_tf_config(TOTAL_STEPS_PER_FILE,
    43                                                 MODEL_DIR,
    44                                                 clear_model_data=False,
    45                                                 keep_checkpoint_max=5):
    46                             
    47    303.7 MiB      0.0 MiB       save_checkpoints_steps=TOTAL_STEPS_PER_FILE * 3
    48                                 # each TFRecord file has NUM_SAMPLE, so for every 3 TFRecord files store the checkpoint
    49                             
    50    303.7 MiB      0.0 MiB       save_summary_steps=TOTAL_STEPS_PER_FILE * 1
    51    303.7 MiB      0.0 MiB       log_step_count_steps=TOTAL_STEPS_PER_FILE * 1
    52                             
    53    303.7 MiB      0.0 MiB       run_config = tf.compat.v1.ConfigProto()
    54    303.7 MiB      0.0 MiB       run_config.gpu_options.allow_growth = True
    55                                 # run_config.gpu_options.per_process_gpu_memory_fraction = 0.50
    56    303.7 MiB      0.0 MiB       run_config.allow_soft_placement = True
    57    303.7 MiB      0.0 MiB       run_config.log_device_placement = False
    58    303.7 MiB      0.0 MiB       model_dir = MODEL_DIR
    59                             
    60    303.7 MiB      0.0 MiB       if clear_model_data:
    61                                     if os.path.exists(model_dir):
    62                                         shutil.rmtree(model_dir)
    63                             
    64    303.7 MiB      0.0 MiB       _run_config = tf.estimator.RunConfig(session_config=run_config,
    65    303.7 MiB      0.0 MiB                                            save_checkpoints_steps=save_checkpoints_steps,
    66    303.7 MiB      0.0 MiB                                            keep_checkpoint_max=keep_checkpoint_max,
    67    303.7 MiB      0.0 MiB                                            save_summary_steps=save_summary_steps,
    68    303.7 MiB      0.0 MiB                                            model_dir=model_dir,
    69    303.7 MiB      0.0 MiB                                            log_step_count_steps=log_step_count_steps)
    70                             
    71    303.7 MiB      0.0 MiB       return _run_config


objgraph growth list start
function                         63331    +63331
dict                             34719    +34719
tuple                            31287    +31287
list                             16026    +16026
cell                             14478    +14478
weakref                           9896     +9896
type                              6925     +6925
getset_descriptor                 6505     +6505
property                          4967     +4967
builtin_function_or_method        3901     +3901
ModuleSpec                        3451     +3451
module                            3448     +3448
wrapper_descriptor                3350     +3350
SourceFileLoader                  3206     +3206
method_descriptor                 3152     +3152
set                               2688     +2688
TFDecorator                       1729     +1729
frozenset                         1543     +1543
ArgSpec                           1224     +1224
_OpInfo                           1224     +1224
method                            1174     +1174
classmethod                       1168     +1168
staticmethod                      1099     +1099
member_descriptor                  944      +944
TagMap                             803      +803
cython_function_or_method          608      +608
fused_cython_function              571      +571
ABCMeta                            562      +562
GeneratedProtocolMessageType       505      +505
MovedAttribute                     496      +496
FileFinder                         463      +463
vectorize                          454      +454
Parameter                          452      +452
NamedTypes                         410      +410
__pyx_scope_struct__with_phil      344      +344
TFModuleWrapper                    344      +344
FontEntry                          335      +335
PointerType                        334      +334
PathMetadata                       332      +332
DistInfoDistribution               325      +325
MiniProduction                     310      +310
MovedModule                        284      +284
NamedType                          251      +251
slice                              228      +228
Enum                               216      +216
And                                213      +213
TagSet                             199      +199
ExtensionFileLoader                192      +192
Tag                                160      +160
OrderedDict                        155      +155
objgraph growth list end







Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
W0903 11:06:09.323728 140674526709568 deprecation.py:506] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W0903 11:06:09.325722 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2019-09-03 11:06:09.357309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-09-03 11:06:09.377345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:09.377753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:06:09.377923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:06:09.378667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:06:09.379332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:06:09.379525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:06:09.380580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:06:09.381391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:06:09.384084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:06:09.384190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:09.384644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:09.385010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
W0903 11:06:31.414942 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
W0903 11:06:31.460271 140674526709568 deprecation.py:323] From /opt/tf_issue_32052/dummy_datasets.py:204: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
I0903 11:06:31.614904 140674526709568 estimator.py:1145] Calling model_fn.
W0903 11:06:39.531893 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/model_fn.py:337: scalar (from tensorflow.python.framework.tensor_shape) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.TensorShape([]).
================================================
    74    305.5 MiB    305.5 MiB   @profile
    75                             def _get_train_spec(TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    76                                 # Estimators expect an input_fn to take no arguments.
    77                                 # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.
    78    305.5 MiB      0.0 MiB       return tf.estimator.TrainSpec(
    79    305.5 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=TRAIN_DATA,
    80                                                                   BATCH_SIZE=BATCH_SIZE,
    81                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    82    305.5 MiB      0.0 MiB           max_steps=max_steps,
    83    305.5 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171    351.7 MiB    351.7 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179    351.7 MiB      0.0 MiB       _num_cores = 4
   180    351.7 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182    351.7 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183    351.7 MiB      0.0 MiB       path = path.replace("//", "/")
   184    352.2 MiB      0.6 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188    352.2 MiB      0.0 MiB       dataset = files.interleave(
   189    352.2 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190    352.2 MiB      0.0 MiB           cycle_length=_num_cores,
   191    352.5 MiB      0.3 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197    352.5 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200    356.8 MiB      4.2 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202    356.8 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204    356.8 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205    356.8 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206    356.8 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18    364.1 MiB    364.1 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20    364.1 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22    364.1 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23    364.1 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24    364.1 MiB      0.0 MiB                                                                    global_step,
    25    364.1 MiB      0.0 MiB                                                                    decay_steps=100,
    26    364.1 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27    364.1 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29    364.1 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30    364.1 MiB      0.0 MiB                                                    beta_1=0.9,
    31    364.1 MiB      0.0 MiB                                                    beta_2=0.999,
    32    364.1 MiB      0.0 MiB                                                    epsilon=1e-7,
    33    364.1 MiB      0.0 MiB                                                    amsgrad=False,
    34    364.1 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36    364.1 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42    364.1 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43    364.1 MiB      0.0 MiB                   loss,
    44    371.5 MiB      7.3 MiB                   tf.compat.v1.trainable_variables())[0]
    45    371.5 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46    371.5 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48    356.8 MiB    356.8 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51    356.8 MiB      0.0 MiB           features = features['data']
    52                             
    53    358.5 MiB      1.7 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54    359.2 MiB      0.7 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55    359.9 MiB      0.8 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56    360.5 MiB      0.5 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57    361.2 MiB      0.8 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58    363.8 MiB      2.6 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59    364.0 MiB      0.2 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60    364.0 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62    364.0 MiB      0.0 MiB           loss = None
    63    364.0 MiB      0.0 MiB           optimizer = None
    64    364.0 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66    364.0 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67    364.0 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68    364.1 MiB      0.1 MiB               loss = mse(logits, label)
    69    364.1 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
I0903 11:06:39.533987 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:06:39.542729 140674526709568 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
W0903 11:06:40.095569 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1486: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
I0903 11:06:41.072338 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:06:41.073139: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-03 11:06:41.095801: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-09-03 11:06:41.096500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a579fac20 executing computations on platform Host. Devices:
2019-09-03 11:06:41.096517: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-03 11:06:41.156836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.157434: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a5782af80 executing computations on platform CUDA. Devices:
2019-09-03 11:06:41.157452: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1060 with Max-Q Design, Compute Capability 6.1
2019-09-03 11:06:41.157603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.158046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:06:41.158084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:06:41.158105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:06:41.158125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:06:41.158145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:06:41.158164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:06:41.158183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:06:41.158203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:06:41.158257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.158699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.159106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:06:41.159140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:06:41.160219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:06:41.160232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:06:41.160238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:06:41.160493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.161034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:41.161579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:06:41.168481 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-6097
W0903 11:06:41.805269 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
I0903 11:06:41.864715 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:06:41.880434 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:06:44.959747 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 6097 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
2019-09-03 11:06:45.307463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
I0903 11:06:45.514431 140674526709568 basic_session_run_hooks.py:262] loss = 35764.35, step = 6097
I0903 11:06:46.146632 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 124.747
I0903 11:06:46.148239 140674526709568 basic_session_run_hooks.py:260] loss = 29961.824, step = 6176 (0.634 sec)
I0903 11:06:46.598706 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 174.769
I0903 11:06:46.600413 140674526709568 basic_session_run_hooks.py:260] loss = 43281.14, step = 6255 (0.452 sec)
I0903 11:06:47.047423 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 6332 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
W0903 11:06:47.067982 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
I0903 11:06:47.213981 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 128.4
I0903 11:06:47.215899 140674526709568 basic_session_run_hooks.py:260] loss = 33279.367, step = 6334 (0.615 sec)
I0903 11:06:47.715090 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 157.636
I0903 11:06:47.716843 140674526709568 basic_session_run_hooks.py:260] loss = 38985.54, step = 6413 (0.501 sec)
I0903 11:06:48.227634 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 154.141
I0903 11:06:48.229543 140674526709568 basic_session_run_hooks.py:260] loss = 42364.37, step = 6492 (0.513 sec)
I0903 11:06:48.737568 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 6566 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:06:48.923102 140674526709568 estimator.py:368] Loss for final step: 32207.666.
E0903 11:06:48.933632 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating[0m
W0903 11:06:49.427762 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
I0903 11:06:49.657977 140674526709568 estimator.py:1145] Calling model_fn.
    71    371.5 MiB      7.3 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73    371.5 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74    371.5 MiB      0.0 MiB               mode=mode,
    75    371.5 MiB      0.0 MiB               predictions=predictions,
    76    371.5 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77    371.5 MiB      0.0 MiB               loss=loss,
    78    371.5 MiB      0.0 MiB               train_op=optimizer,
    79    371.5 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    96    305.5 MiB    305.5 MiB   @profile
    97                             def train(estimator, TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    98    305.5 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,
    99    305.5 MiB      0.0 MiB                                    max_steps=max_steps,
   100    305.5 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,
   101    305.5 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   102    305.5 MiB      0.0 MiB       estimator.train(
   103    305.5 MiB      0.0 MiB           input_fn=train_spec.input_fn,
   104    305.5 MiB      0.0 MiB           hooks=train_spec.hooks,
   105   1307.3 MiB   1001.8 MiB           max_steps=train_spec.max_steps)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    86   1307.3 MiB   1307.3 MiB   @profile
    87                             def _get_eval_spec(VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps):
    88   1307.3 MiB      0.0 MiB       return tf.estimator.EvalSpec(
    89   1307.3 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=VAL_DATA,
    90                                                                   BATCH_SIZE=BATCH_SIZE,
    91                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    92   1307.3 MiB      0.0 MiB           steps=steps,
    93   1307.3 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1307.2 MiB   1307.2 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1307.2 MiB      0.0 MiB       _num_cores = 4
   180   1307.2 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1307.2 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1307.2 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1307.2 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1307.2 MiB      0.0 MiB       dataset = files.interleave(
   189   1307.2 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1307.2 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1307.2 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1307.2 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1307.2 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1307.2 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1307.3 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1307.3 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1307.3 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1309.3 MiB   1309.3 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1309.3 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1309.3 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1309.3 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1309.3 MiB      0.0 MiB                                                                    global_step,
    25   1309.3 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1309.3 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1309.3 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1309.3 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1309.3 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1309.3 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1309.3 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1309.3 MiB      0.0 MiB                                                    amsgrad=False,
    34   1309.3 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1309.3 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1309.3 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1309.3 MiB      0.0 MiB                   loss,
    44   1314.2 MiB      4.9 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1314.2 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1314.2 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1307.3 MiB   1307.3 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1307.3 MiB      0.0 MiB           features = features['data']
    52                             
    53   1307.5 MiB      0.3 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
I0903 11:06:56.402467 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:06:56.596964 140674526709568 evaluation.py:255] Starting evaluation at 2019-09-03T11:06:56Z
I0903 11:06:57.930755 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:06:57.932067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:57.932741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:06:57.932831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:06:57.932860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:06:57.932881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:06:57.932901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:06:57.932920: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:06:57.932939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:06:57.932960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:06:57.933052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:57.933673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:57.934161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:06:57.934193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:06:57.934203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:06:57.934209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:06:57.934493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:57.935071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:06:57.935678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:06:57.936698 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-6566
I0903 11:06:58.122797 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:06:58.148952 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:06:59.540254 140674526709568 evaluation.py:275] Finished evaluation at 2019-09-03-11:06:59
I0903 11:06:59.541374 140674526709568 estimator.py:2039] Saving dict for global step 6566: global_step = 6566, loss = 36906.95
I0903 11:06:59.965190 140674526709568 estimator.py:2099] Saving 'checkpoint_path' summary for global step 6566: /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-6566
 20%|â–ˆâ–ˆ        | 1/5 [00:52<03:31, 52.98s/it]E0903 11:07:02.254678 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:07:02.255552 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:07:02.255950 140674526709568 print_helper.py:77] [93mMemory used is 1316.85546875[0m
I0903 11:07:02.256323 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
E0903 11:07:02.257076 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training[0m
W0903 11:07:02.817684 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
    54   1307.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1307.8 MiB      0.3 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1308.3 MiB      0.5 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1308.5 MiB      0.2 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1308.8 MiB      0.3 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1309.0 MiB      0.2 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1309.3 MiB      0.3 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1309.3 MiB      0.0 MiB           loss = None
    63   1309.3 MiB      0.0 MiB           optimizer = None
    64   1309.3 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1309.3 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1309.3 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1309.3 MiB      0.0 MiB               loss = mse(logits, label)
    69   1309.3 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1314.2 MiB      4.9 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1314.2 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1314.2 MiB      0.0 MiB               mode=mode,
    75   1314.2 MiB      0.0 MiB               predictions=predictions,
    76   1314.2 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1314.2 MiB      0.0 MiB               loss=loss,
    78   1314.2 MiB      0.0 MiB               train_op=optimizer,
    79   1314.2 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   108   1307.3 MiB   1307.3 MiB   @profile
   109                             def evaluate(estimator, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps=None, checkpoint_path=None):
   110   1307.3 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA, steps=steps,
   111   1307.3 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,
   112   1307.3 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   113   1307.3 MiB      0.0 MiB       estimator.evaluate(
   114   1307.3 MiB      0.0 MiB           input_fn=eval_spec.input_fn,
   115   1307.3 MiB      0.0 MiB           steps=eval_spec.steps,
   116   1307.3 MiB      0.0 MiB           hooks=eval_spec.hooks,
   117   1314.9 MiB      7.6 MiB           checkpoint_path=checkpoint_path)


objgraph growth list start
weakref                         11673     +1777
builtin_function_or_method       5641     +1740
LineLocation                      203      +203
dict                            34889      +170
OriginInfo                        110      +110
Location                          110      +110
tuple                           31384       +97
function                        63402       +71
list                            16065       +39
cell                            14507       +29
set                              2715       +27
module                           3464       +16
ModuleSpec                       3466       +15
SourceFileLoader                 3221       +15
type                             6938       +13
TextIOWrapper                      16       +12
IncrementalEncoder                 15       +12
FileIO                             16       +12
BufferedWriter                     15       +12
deque                              26       +12
_TemporaryFileWrapper              12       +12
_TemporaryFileCloser               12       +12
_ConvertedEntityFactoryInfo        12       +12
Condition                          42       +10
ConversionOptions                  11       +10
method                           1182        +8
Event                               9        +4
DeviceSpecV1                        5        +4
MergeDevice                         4        +4
PhysicalDevice                      4        +4
MessageMeta                         3        +3
property                         4969        +2
CodecInfo                           3        +2
Queue                               2        +2
_EventLoggerThread                  2        +2
EventsWriter                        2        +2
FileWriter                          2        +2
EventFileWriter                     2        +2
frozenset                        1544        +1
frame                              45        +1
Random                              3        +1
generator                           3        +1
_RandomNameSequence                 2        +1
TMonitor                            3        +1
tqdm                                1        +1
ReplicaContext                      1        +1
_InReplicaThreadMode                1        +1
FunctionCallOptions                 1        +1
_DefaultDistributionStrategy        1        +1
_DefaultReplicaThreadMode           1        +1
objgraph growth list end







Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    74   1316.9 MiB   1316.9 MiB   @profile
    75                             def _get_train_spec(TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    76                                 # Estimators expect an input_fn to take no arguments.
    77                                 # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.
    78   1316.9 MiB      0.0 MiB       return tf.estimator.TrainSpec(
    79   1316.9 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=TRAIN_DATA,
    80                                                                   BATCH_SIZE=BATCH_SIZE,
    81                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    82   1316.9 MiB      0.0 MiB           max_steps=max_steps,
    83   1316.9 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1316.9 MiB   1316.9 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1316.9 MiB      0.0 MiB       _num_cores = 4
   180   1316.9 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1316.9 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1316.9 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1316.9 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1316.9 MiB      0.0 MiB       dataset = files.interleave(
   189   1316.9 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1316.9 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1316.9 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
I0903 11:07:03.080204 140674526709568 estimator.py:1145] Calling model_fn.
I0903 11:07:11.577228 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:07:11.588212 140674526709568 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0903 11:07:13.116394 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:07:13.117507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:13.118006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:07:13.118055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:07:13.118073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:07:13.118089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:07:13.118105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:07:13.118120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:07:13.118136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:07:13.118152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:07:13.118222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:13.118739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:13.119218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:07:13.119248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:07:13.119256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:07:13.119262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:07:13.119524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:13.120176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:13.120658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:07:13.124297 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-6566
I0903 11:07:13.274671 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:07:13.289921 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:07:17.118083 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 6566 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:17.705317 140674526709568 basic_session_run_hooks.py:262] loss = 32244.91, step = 6566
I0903 11:07:18.708386 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 78.5894
I0903 11:07:18.711117 140674526709568 basic_session_run_hooks.py:260] loss = 27267.045, step = 6645 (1.006 sec)
I0903 11:07:19.433320 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 109.028
I0903 11:07:19.438884 140674526709568 basic_session_run_hooks.py:260] loss = 31060.096, step = 6724 (0.728 sec)
I0903 11:07:20.120680 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 6801 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:20.342286 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 86.8737
I0903 11:07:20.344471 140674526709568 basic_session_run_hooks.py:260] loss = 40406.53, step = 6803 (0.906 sec)
I0903 11:07:20.976942 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 124.474
I0903 11:07:20.979174 140674526709568 basic_session_run_hooks.py:260] loss = 38742.96, step = 6882 (0.635 sec)
I0903 11:07:21.610153 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 124.783
I0903 11:07:21.613576 140674526709568 basic_session_run_hooks.py:260] loss = 31270.23, step = 6961 (0.634 sec)
I0903 11:07:22.290834 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7035 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:22.510686 140674526709568 estimator.py:368] Loss for final step: 47180.82.
E0903 11:07:22.521624 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating[0m
W0903 11:07:23.005061 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
   196                             
   197   1316.9 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1316.9 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1316.9 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1316.9 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1316.9 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1316.9 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1316.6 MiB   1316.6 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1316.6 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1316.6 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1316.6 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1316.6 MiB      0.0 MiB                                                                    global_step,
    25   1316.6 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1316.6 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1316.6 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1316.6 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1316.6 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1316.6 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1316.6 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1316.6 MiB      0.0 MiB                                                    amsgrad=False,
    34   1316.6 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1316.6 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1316.6 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1316.6 MiB      0.0 MiB                   loss,
    44   1316.6 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1316.6 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1316.6 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1316.6 MiB   1316.6 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1316.6 MiB      0.0 MiB           features = features['data']
    52                             
    53   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1316.6 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1316.6 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1316.6 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1316.6 MiB      0.0 MiB           loss = None
    63   1316.6 MiB      0.0 MiB           optimizer = None
    64   1316.6 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1316.6 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1316.6 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1316.6 MiB      0.0 MiB               loss = mse(logits, label)
    69   1316.6 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1316.6 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1316.6 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1316.6 MiB      0.0 MiB               mode=mode,
    75   1316.6 MiB      0.0 MiB               predictions=predictions,
    76   1316.6 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1316.6 MiB      0.0 MiB               loss=loss,
    78   1316.6 MiB      0.0 MiB               train_op=optimizer,
    79   1316.6 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    96   1316.9 MiB   1316.9 MiB   @profile
    97                             def train(estimator, TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    98   1316.9 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,
    99   1316.9 MiB      0.0 MiB                                    max_steps=max_steps,
   100   1316.9 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,
   101   1316.9 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   102   1316.9 MiB      0.0 MiB       estimator.train(
   103   1316.9 MiB      0.0 MiB           input_fn=train_spec.input_fn,
   104   1316.9 MiB      0.0 MiB           hooks=train_spec.hooks,
   105   1336.9 MiB     20.1 MiB           max_steps=train_spec.max_steps)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    86   1336.9 MiB   1336.9 MiB   @profile
    87                             def _get_eval_spec(VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps):
    88   1336.9 MiB      0.0 MiB       return tf.estimator.EvalSpec(
    89   1336.9 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=VAL_DATA,
    90                                                                   BATCH_SIZE=BATCH_SIZE,
    91                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    92   1336.9 MiB      0.0 MiB           steps=steps,
    93   1336.9 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1336.9 MiB   1336.9 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
I0903 11:07:23.274848 140674526709568 estimator.py:1145] Calling model_fn.
I0903 11:07:30.818688 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:07:31.039608 140674526709568 evaluation.py:255] Starting evaluation at 2019-09-03T11:07:31Z
I0903 11:07:32.329627 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:07:32.330620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:32.331042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:07:32.331081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:07:32.331094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:07:32.331105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:07:32.331116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:07:32.331127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:07:32.331138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:07:32.331150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:07:32.331200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:32.331596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:32.331950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:07:32.331969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:07:32.331973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:07:32.331977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:07:32.332149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:32.332539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:32.332903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:07:32.333566 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7035
I0903 11:07:32.449357 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:07:32.468119 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:07:33.348274 140674526709568 evaluation.py:275] Finished evaluation at 2019-09-03-11:07:33
I0903 11:07:33.348857 140674526709568 estimator.py:2039] Saving dict for global step 7035: global_step = 7035, loss = 36906.95
I0903 11:07:33.349773 140674526709568 estimator.py:2099] Saving 'checkpoint_path' summary for global step 7035: /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7035
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1336.9 MiB      0.0 MiB       _num_cores = 4
   180   1336.9 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1336.9 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1336.9 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1336.9 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1336.9 MiB      0.0 MiB       dataset = files.interleave(
   189   1336.9 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1336.9 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1336.9 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1336.9 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1336.9 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1336.9 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1336.9 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1336.9 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1336.9 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1336.9 MiB   1336.9 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1336.9 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1336.9 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1336.9 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1336.9 MiB      0.0 MiB                                                                    global_step,
    25   1336.9 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1336.9 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1336.9 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1336.9 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1336.9 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1336.9 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1336.9 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1336.9 MiB      0.0 MiB                                                    amsgrad=False,
    34   1336.9 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1336.9 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1336.9 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1336.9 MiB      0.0 MiB                   loss,
    44   1336.9 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1336.9 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1336.9 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1336.9 MiB   1336.9 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1336.9 MiB      0.0 MiB           features = features['data']
    52                             
    53   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1336.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1336.9 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1336.9 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1336.9 MiB      0.0 MiB           loss = None
    63   1336.9 MiB      0.0 MiB           optimizer = None
    64   1336.9 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1336.9 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1336.9 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1336.9 MiB      0.0 MiB               loss = mse(logits, label)
    69   1336.9 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1336.9 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1336.9 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1336.9 MiB      0.0 MiB               mode=mode,
    75   1336.9 MiB      0.0 MiB               predictions=predictions,
    76   1336.9 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1336.9 MiB      0.0 MiB               loss=loss,
    78   1336.9 MiB      0.0 MiB               train_op=optimizer,
    79   1336.9 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   108   1336.9 MiB   1336.9 MiB   @profile
   109                             def evaluate(estimator, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps=None, checkpoint_path=None):
   110   1336.9 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA, steps=steps,
   111   1336.9 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,
   112   1336.9 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   113   1336.9 MiB      0.0 MiB       estimator.evaluate(
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:25<02:20, 46.93s/it]E0903 11:07:35.080800 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:07:35.081539 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:07:35.081853 140674526709568 print_helper.py:77] [93mMemory used is 1336.32421875[0m
I0903 11:07:35.082139 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
E0903 11:07:35.082714 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training[0m
W0903 11:07:35.518430 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
I0903 11:07:35.720260 140674526709568 estimator.py:1145] Calling model_fn.
   114   1336.9 MiB      0.0 MiB           input_fn=eval_spec.input_fn,
   115   1336.9 MiB      0.0 MiB           steps=eval_spec.steps,
   116   1336.9 MiB      0.0 MiB           hooks=eval_spec.hooks,
   117   1337.3 MiB      0.4 MiB           checkpoint_path=checkpoint_path)


objgraph growth list start
objgraph growth list end







Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    74   1336.3 MiB   1336.3 MiB   @profile
    75                             def _get_train_spec(TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    76                                 # Estimators expect an input_fn to take no arguments.
    77                                 # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.
    78   1336.3 MiB      0.0 MiB       return tf.estimator.TrainSpec(
    79   1336.3 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=TRAIN_DATA,
    80                                                                   BATCH_SIZE=BATCH_SIZE,
    81                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    82   1336.3 MiB      0.0 MiB           max_steps=max_steps,
    83   1336.3 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1336.3 MiB   1336.3 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1336.3 MiB      0.0 MiB       _num_cores = 4
   180   1336.3 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1336.3 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1336.3 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1336.3 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1336.3 MiB      0.0 MiB       dataset = files.interleave(
   189   1336.3 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1336.3 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1336.3 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1336.3 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1336.3 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1336.3 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1336.3 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1336.3 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1336.3 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1336.3 MiB   1336.3 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1336.3 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1336.3 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1336.3 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1336.3 MiB      0.0 MiB                                                                    global_step,
    25   1336.3 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1336.3 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1336.3 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1336.3 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1336.3 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1336.3 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1336.3 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1336.3 MiB      0.0 MiB                                                    amsgrad=False,
    34   1336.3 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1336.3 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1336.3 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1336.3 MiB      0.0 MiB                   loss,
    44   1336.3 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1336.3 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1336.3 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1336.3 MiB   1336.3 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1336.3 MiB      0.0 MiB           features = features['data']
    52                             
    53   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1336.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1336.3 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1336.3 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1336.3 MiB      0.0 MiB           loss = None
    63   1336.3 MiB      0.0 MiB           optimizer = None
    64   1336.3 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
I0903 11:07:42.019800 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:07:42.028230 140674526709568 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0903 11:07:43.193238 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:07:43.194078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:43.194500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:07:43.194540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:07:43.194553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:07:43.194564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:07:43.194576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:07:43.194587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:07:43.194598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:07:43.194610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:07:43.194661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:43.195066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:43.195433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:07:43.195452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:07:43.195457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:07:43.195461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:07:43.195649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:43.196054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:43.196428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:07:43.199180 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7035
I0903 11:07:43.309006 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:07:43.321367 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:07:46.126281 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7035 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:46.468676 140674526709568 basic_session_run_hooks.py:262] loss = 31464.145, step = 7035
I0903 11:07:47.073158 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 130.498
I0903 11:07:47.075075 140674526709568 basic_session_run_hooks.py:260] loss = 27291.664, step = 7114 (0.606 sec)
I0903 11:07:47.568816 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 159.381
I0903 11:07:47.570677 140674526709568 basic_session_run_hooks.py:260] loss = 35402.883, step = 7193 (0.496 sec)
I0903 11:07:48.035743 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7270 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:48.986823 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 55.7102
I0903 11:07:48.988381 140674526709568 basic_session_run_hooks.py:260] loss = 37093.82, step = 7272 (1.418 sec)
I0903 11:07:49.396920 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 192.639
I0903 11:07:49.398487 140674526709568 basic_session_run_hooks.py:260] loss = 41121.293, step = 7351 (0.410 sec)
I0903 11:07:49.844875 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 176.373
I0903 11:07:49.846774 140674526709568 basic_session_run_hooks.py:260] loss = 41335.406, step = 7430 (0.448 sec)
I0903 11:07:50.352533 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7504 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:07:50.527738 140674526709568 estimator.py:368] Loss for final step: 46568.184.
E0903 11:07:50.536444 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating[0m
W0903 11:07:50.950875 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
I0903 11:07:51.153657 140674526709568 estimator.py:1145] Calling model_fn.
    65                             
    66   1336.3 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1336.3 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1336.3 MiB      0.0 MiB               loss = mse(logits, label)
    69   1336.3 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1336.3 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1336.3 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1336.3 MiB      0.0 MiB               mode=mode,
    75   1336.3 MiB      0.0 MiB               predictions=predictions,
    76   1336.3 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1336.3 MiB      0.0 MiB               loss=loss,
    78   1336.3 MiB      0.0 MiB               train_op=optimizer,
    79   1336.3 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    96   1336.3 MiB   1336.3 MiB   @profile
    97                             def train(estimator, TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    98   1336.3 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,
    99   1336.3 MiB      0.0 MiB                                    max_steps=max_steps,
   100   1336.3 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,
   101   1336.3 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   102   1336.3 MiB      0.0 MiB       estimator.train(
   103   1336.3 MiB      0.0 MiB           input_fn=train_spec.input_fn,
   104   1336.3 MiB      0.0 MiB           hooks=train_spec.hooks,
   105   1369.9 MiB     33.6 MiB           max_steps=train_spec.max_steps)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    86   1369.9 MiB   1369.9 MiB   @profile
    87                             def _get_eval_spec(VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps):
    88   1369.9 MiB      0.0 MiB       return tf.estimator.EvalSpec(
    89   1369.9 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=VAL_DATA,
    90                                                                   BATCH_SIZE=BATCH_SIZE,
    91                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    92   1369.9 MiB      0.0 MiB           steps=steps,
    93   1369.9 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1369.9 MiB   1369.9 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1369.9 MiB      0.0 MiB       _num_cores = 4
   180   1369.9 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1369.9 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1369.9 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1369.9 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1369.9 MiB      0.0 MiB       dataset = files.interleave(
   189   1369.9 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1369.9 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1369.9 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1369.9 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1369.9 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1369.9 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1369.9 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1369.9 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1369.9 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1370.4 MiB   1370.4 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1370.4 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1370.4 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1370.4 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1370.4 MiB      0.0 MiB                                                                    global_step,
    25   1370.4 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1370.4 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1370.4 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1370.4 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1370.4 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1370.4 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1370.4 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1370.4 MiB      0.0 MiB                                                    amsgrad=False,
    34   1370.4 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1370.4 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1370.4 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1370.4 MiB      0.0 MiB                   loss,
    44   1370.4 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1370.4 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1370.4 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
I0903 11:07:57.526886 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:07:57.710038 140674526709568 evaluation.py:255] Starting evaluation at 2019-09-03T11:07:57Z
I0903 11:07:58.918140 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:07:58.919116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:58.919553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:07:58.919594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:07:58.919607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:07:58.919619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:07:58.919631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:07:58.919643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:07:58.919655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:07:58.919667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:07:58.919720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:58.920141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:58.920521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:07:58.920542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:07:58.920547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:07:58.920552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:07:58.920739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:58.921158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:07:58.921548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:07:58.922267 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7504
I0903 11:07:59.037542 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:07:59.054389 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:07:59.962704 140674526709568 evaluation.py:275] Finished evaluation at 2019-09-03-11:07:59
I0903 11:07:59.963366 140674526709568 estimator.py:2039] Saving dict for global step 7504: global_step = 7504, loss = 36906.95
I0903 11:07:59.964392 140674526709568 estimator.py:2099] Saving 'checkpoint_path' summary for global step 7504: /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7504
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:52<01:21, 40.83s/it]E0903 11:08:01.675561 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:08:01.676215 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:08:01.676505 140674526709568 print_helper.py:77] [93mMemory used is 1369.87890625[0m
I0903 11:08:01.676777 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
E0903 11:08:01.677327 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training[0m
W0903 11:08:02.106656 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
I0903 11:08:02.317486 140674526709568 estimator.py:1145] Calling model_fn.
================================================
    48   1369.9 MiB   1369.9 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1369.9 MiB      0.0 MiB           features = features['data']
    52                             
    53   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1370.2 MiB      0.3 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1370.4 MiB      0.3 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1370.4 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1370.4 MiB      0.0 MiB           loss = None
    63   1370.4 MiB      0.0 MiB           optimizer = None
    64   1370.4 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1370.4 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1370.4 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1370.4 MiB      0.0 MiB               loss = mse(logits, label)
    69   1370.4 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1370.4 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1370.4 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1370.4 MiB      0.0 MiB               mode=mode,
    75   1370.4 MiB      0.0 MiB               predictions=predictions,
    76   1370.4 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1370.4 MiB      0.0 MiB               loss=loss,
    78   1370.4 MiB      0.0 MiB               train_op=optimizer,
    79   1370.4 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   108   1369.9 MiB   1369.9 MiB   @profile
   109                             def evaluate(estimator, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps=None, checkpoint_path=None):
   110   1369.9 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA, steps=steps,
   111   1369.9 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,
   112   1369.9 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   113   1369.9 MiB      0.0 MiB       estimator.evaluate(
   114   1369.9 MiB      0.0 MiB           input_fn=eval_spec.input_fn,
   115   1369.9 MiB      0.0 MiB           steps=eval_spec.steps,
   116   1369.9 MiB      0.0 MiB           hooks=eval_spec.hooks,
   117   1370.9 MiB      0.9 MiB           checkpoint_path=checkpoint_path)


objgraph growth list start
objgraph growth list end







Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    74   1369.9 MiB   1369.9 MiB   @profile
    75                             def _get_train_spec(TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    76                                 # Estimators expect an input_fn to take no arguments.
    77                                 # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.
    78   1369.9 MiB      0.0 MiB       return tf.estimator.TrainSpec(
    79   1369.9 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=TRAIN_DATA,
    80                                                                   BATCH_SIZE=BATCH_SIZE,
    81                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    82   1369.9 MiB      0.0 MiB           max_steps=max_steps,
    83   1369.9 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1369.9 MiB   1369.9 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1369.9 MiB      0.0 MiB       _num_cores = 4
   180   1369.9 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1369.9 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1369.9 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1369.9 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1369.9 MiB      0.0 MiB       dataset = files.interleave(
   189   1369.9 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1369.9 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1369.9 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1369.9 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1369.9 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1369.9 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1369.9 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1369.9 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1369.9 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1369.9 MiB   1369.9 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1369.9 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1369.9 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1369.9 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1369.9 MiB      0.0 MiB                                                                    global_step,
    25   1369.9 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1369.9 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1369.9 MiB      0.0 MiB                                                                    staircase=True)
I0903 11:08:08.682718 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:08:08.691788 140674526709568 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0903 11:08:09.852781 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:08:09.853647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:09.854080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:08:09.854122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:08:09.854135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:08:09.854149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:08:09.854161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:08:09.854180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:08:09.854198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:08:09.854212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:08:09.854274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:09.854693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:09.855069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:08:09.855092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:08:09.855097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:08:09.855101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:08:09.855295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:09.855721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:09.856107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:08:09.858911 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7504
I0903 11:08:09.972582 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:08:09.984697 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:08:12.960696 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7504 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:13.352960 140674526709568 basic_session_run_hooks.py:262] loss = 27945.598, step = 7504
I0903 11:08:14.066490 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 110.56
I0903 11:08:14.068858 140674526709568 basic_session_run_hooks.py:260] loss = 23079.809, step = 7583 (0.716 sec)
I0903 11:08:14.662573 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 132.534
I0903 11:08:14.664874 140674526709568 basic_session_run_hooks.py:260] loss = 32793.4, step = 7662 (0.596 sec)
I0903 11:08:15.226783 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7739 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:15.420772 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 104.191
I0903 11:08:15.422748 140674526709568 basic_session_run_hooks.py:260] loss = 37674.453, step = 7741 (0.758 sec)
I0903 11:08:15.953966 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 148.165
I0903 11:08:15.956123 140674526709568 basic_session_run_hooks.py:260] loss = 44274.47, step = 7820 (0.533 sec)
I0903 11:08:16.505102 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 143.344
I0903 11:08:16.507384 140674526709568 basic_session_run_hooks.py:260] loss = 39606.9, step = 7899 (0.551 sec)
I0903 11:08:17.055861 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7973 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:17.246387 140674526709568 estimator.py:368] Loss for final step: 47944.637.
E0903 11:08:17.248556 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating[0m
W0903 11:08:17.659296 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
    28                             
    29   1369.9 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1369.9 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1369.9 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1369.9 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1369.9 MiB      0.0 MiB                                                    amsgrad=False,
    34   1369.9 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1369.9 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1369.9 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1369.9 MiB      0.0 MiB                   loss,
    44   1369.9 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1369.9 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1369.9 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1369.9 MiB   1369.9 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1369.9 MiB      0.0 MiB           features = features['data']
    52                             
    53   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1369.9 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1369.9 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1369.9 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1369.9 MiB      0.0 MiB           loss = None
    63   1369.9 MiB      0.0 MiB           optimizer = None
    64   1369.9 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1369.9 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1369.9 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1369.9 MiB      0.0 MiB               loss = mse(logits, label)
    69   1369.9 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1369.9 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1369.9 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1369.9 MiB      0.0 MiB               mode=mode,
    75   1369.9 MiB      0.0 MiB               predictions=predictions,
    76   1369.9 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1369.9 MiB      0.0 MiB               loss=loss,
    78   1369.9 MiB      0.0 MiB               train_op=optimizer,
    79   1369.9 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    96   1369.9 MiB   1369.9 MiB   @profile
    97                             def train(estimator, TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    98   1369.9 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,
    99   1369.9 MiB      0.0 MiB                                    max_steps=max_steps,
   100   1369.9 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,
   101   1369.9 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   102   1369.9 MiB      0.0 MiB       estimator.train(
   103   1369.9 MiB      0.0 MiB           input_fn=train_spec.input_fn,
   104   1369.9 MiB      0.0 MiB           hooks=train_spec.hooks,
   105   1387.3 MiB     17.5 MiB           max_steps=train_spec.max_steps)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    86   1387.3 MiB   1387.3 MiB   @profile
    87                             def _get_eval_spec(VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps):
    88   1387.3 MiB      0.0 MiB       return tf.estimator.EvalSpec(
    89   1387.3 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=VAL_DATA,
    90                                                                   BATCH_SIZE=BATCH_SIZE,
    91                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    92   1387.3 MiB      0.0 MiB           steps=steps,
    93   1387.3 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1387.3 MiB   1387.3 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1387.3 MiB      0.0 MiB       _num_cores = 4
   180   1387.3 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1387.3 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1387.3 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1387.3 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1387.3 MiB      0.0 MiB       dataset = files.interleave(
   189   1387.3 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1387.3 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1387.3 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1387.3 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1387.3 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1387.3 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
I0903 11:08:17.866458 140674526709568 estimator.py:1145] Calling model_fn.
I0903 11:08:24.147080 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:08:24.329647 140674526709568 evaluation.py:255] Starting evaluation at 2019-09-03T11:08:24Z
I0903 11:08:25.488286 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:08:25.489152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:25.489636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:08:25.489681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:08:25.489701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:08:25.489720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:08:25.489741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:08:25.489759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:08:25.489777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:08:25.489796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:08:25.489857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:25.490276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:25.490655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:08:25.490679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:08:25.490687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:08:25.490694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:08:25.490885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:25.491302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:25.491695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:08:25.492462 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7973
I0903 11:08:25.606134 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:08:25.622901 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:08:26.552208 140674526709568 evaluation.py:275] Finished evaluation at 2019-09-03-11:08:26
I0903 11:08:26.552904 140674526709568 estimator.py:2039] Saving dict for global step 7973: global_step = 7973, loss = 36906.95
I0903 11:08:26.553994 140674526709568 estimator.py:2099] Saving 'checkpoint_path' summary for global step 7973: /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7973
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:18<00:36, 36.53s/it]E0903 11:08:28.180122 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:08:28.180748 140674526709568 print_helper.py:59] [92m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[0m
W0903 11:08:28.181035 140674526709568 print_helper.py:77] [93mMemory used is 1388.09375[0m
I0903 11:08:28.181298 140674526709568 print_helper.py:59] [92m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<[0m
E0903 11:08:28.181822 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training[0m
W0903 11:08:28.621055 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1387.3 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1387.3 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1387.3 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1388.1 MiB   1388.1 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1388.1 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1388.1 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1388.1 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1388.1 MiB      0.0 MiB                                                                    global_step,
    25   1388.1 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1388.1 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1388.1 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1388.1 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1388.1 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1388.1 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1388.1 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1388.1 MiB      0.0 MiB                                                    amsgrad=False,
    34   1388.1 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1388.1 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1388.1 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1388.1 MiB      0.0 MiB                   loss,
    44   1388.1 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1388.1 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1388.1 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1387.3 MiB   1387.3 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1387.3 MiB      0.0 MiB           features = features['data']
    52                             
    53   1387.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1387.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1387.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1387.3 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1387.6 MiB      0.3 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1387.8 MiB      0.3 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1388.1 MiB      0.3 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1388.1 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1388.1 MiB      0.0 MiB           loss = None
    63   1388.1 MiB      0.0 MiB           optimizer = None
    64   1388.1 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1388.1 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1388.1 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1388.1 MiB      0.0 MiB               loss = mse(logits, label)
    69   1388.1 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1388.1 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1388.1 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1388.1 MiB      0.0 MiB               mode=mode,
    75   1388.1 MiB      0.0 MiB               predictions=predictions,
    76   1388.1 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1388.1 MiB      0.0 MiB               loss=loss,
    78   1388.1 MiB      0.0 MiB               train_op=optimizer,
    79   1388.1 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   108   1387.3 MiB   1387.3 MiB   @profile
   109                             def evaluate(estimator, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps=None, checkpoint_path=None):
   110   1387.3 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA, steps=steps,
   111   1387.3 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,
   112   1387.3 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   113   1387.3 MiB      0.0 MiB       estimator.evaluate(
   114   1387.3 MiB      0.0 MiB           input_fn=eval_spec.input_fn,
   115   1387.3 MiB      0.0 MiB           steps=eval_spec.steps,
   116   1387.3 MiB      0.0 MiB           hooks=eval_spec.hooks,
   117   1388.3 MiB      1.0 MiB           checkpoint_path=checkpoint_path)


objgraph growth list start
objgraph growth list end







Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    74   1388.1 MiB   1388.1 MiB   @profile
    75                             def _get_train_spec(TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    76                                 # Estimators expect an input_fn to take no arguments.
    77                                 # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.
    78   1388.1 MiB      0.0 MiB       return tf.estimator.TrainSpec(
    79   1388.1 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=TRAIN_DATA,
    80                                                                   BATCH_SIZE=BATCH_SIZE,
    81                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    82   1388.1 MiB      0.0 MiB           max_steps=max_steps,
    83   1388.1 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1388.1 MiB   1388.1 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1388.1 MiB      0.0 MiB       _num_cores = 4
I0903 11:08:28.823083 140674526709568 estimator.py:1145] Calling model_fn.
I0903 11:08:35.096721 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:08:35.105438 140674526709568 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0903 11:08:36.321125 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:08:36.321999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:36.322424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:08:36.322473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:08:36.322488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:08:36.322503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:08:36.322518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:08:36.322533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:08:36.322548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:08:36.322563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:08:36.322620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:36.323037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:36.323452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:08:36.323473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:08:36.323479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:08:36.323483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:08:36.323680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:36.324096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:36.324481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:08:36.327313 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-7973
I0903 11:08:36.446572 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:08:36.458591 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:08:39.372658 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 7973 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:39.779047 140674526709568 basic_session_run_hooks.py:262] loss = 31215.8, step = 7973
I0903 11:08:40.415884 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 123.86
I0903 11:08:40.417824 140674526709568 basic_session_run_hooks.py:260] loss = 28177.145, step = 8052 (0.639 sec)
I0903 11:08:40.919240 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 156.947
I0903 11:08:40.921174 140674526709568 basic_session_run_hooks.py:260] loss = 33170.64, step = 8131 (0.503 sec)
I0903 11:08:41.425531 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 8208 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:41.593732 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 117.135
I0903 11:08:41.595530 140674526709568 basic_session_run_hooks.py:260] loss = 40832.266, step = 8210 (0.674 sec)
I0903 11:08:42.099003 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 156.384
I0903 11:08:42.102278 140674526709568 basic_session_run_hooks.py:260] loss = 40150.95, step = 8289 (0.507 sec)
I0903 11:08:42.620545 140674526709568 basic_session_run_hooks.py:692] global_step/sec: 151.46
I0903 11:08:42.623133 140674526709568 basic_session_run_hooks.py:260] loss = 35246.438, step = 8368 (0.521 sec)
I0903 11:08:43.117830 140674526709568 basic_session_run_hooks.py:606] Saving checkpoints for 8442 into /opt/tf_issue_32052/data/fwd_nnet/model.ckpt.
I0903 11:08:43.285359 140674526709568 estimator.py:368] Loss for final step: 43745.758.
   180   1388.1 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1388.1 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1388.1 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1388.1 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1388.1 MiB      0.0 MiB       dataset = files.interleave(
   189   1388.1 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1388.1 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1388.1 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1388.1 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1388.1 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1388.1 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1388.1 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1388.1 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1388.1 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1387.8 MiB   1387.8 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1387.8 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1387.8 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1387.8 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1387.8 MiB      0.0 MiB                                                                    global_step,
    25   1387.8 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1387.8 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1387.8 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1387.8 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1387.8 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1387.8 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1387.8 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1387.8 MiB      0.0 MiB                                                    amsgrad=False,
    34   1387.8 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1387.8 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1387.8 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1387.8 MiB      0.0 MiB                   loss,
    44   1387.8 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1387.8 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1387.8 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1387.8 MiB   1387.8 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1387.8 MiB      0.0 MiB           features = features['data']
    52                             
    53   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1387.8 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1387.8 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1387.8 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1387.8 MiB      0.0 MiB           loss = None
    63   1387.8 MiB      0.0 MiB           optimizer = None
    64   1387.8 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1387.8 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1387.8 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1387.8 MiB      0.0 MiB               loss = mse(logits, label)
    69   1387.8 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1387.8 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1387.8 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1387.8 MiB      0.0 MiB               mode=mode,
    75   1387.8 MiB      0.0 MiB               predictions=predictions,
    76   1387.8 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1387.8 MiB      0.0 MiB               loss=loss,
    78   1387.8 MiB      0.0 MiB               train_op=optimizer,
    79   1387.8 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    96   1388.1 MiB   1388.1 MiB   @profile
    97                             def train(estimator, TRAIN_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None):
    98   1388.1 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,
    99   1388.1 MiB      0.0 MiB                                    max_steps=max_steps,
   100   1388.1 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,
   101   1388.1 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   102   1388.1 MiB      0.0 MiB       estimator.train(
   103   1388.1 MiB      0.0 MiB           input_fn=train_spec.input_fn,
   104   1388.1 MiB      0.0 MiB           hooks=train_spec.hooks,
   105   1388.5 MiB      0.4 MiB           max_steps=train_spec.max_steps)
E0903 11:08:43.294635 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating[0m
W0903 11:08:43.669695 140674526709568 ag_logging.py:146] Entity <bound method CodeMap.items of {<code object numpy_array_decode at 0x7ff0c29bee40, file "/opt/tf_issue_32052/dummy_datasets.py", line 126>: {}}> appears to be a generator function. It will not be converted by AutoGraph.
I0903 11:08:43.864392 140674526709568 estimator.py:1145] Calling model_fn.


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
    86   1388.5 MiB   1388.5 MiB   @profile
    87                             def _get_eval_spec(VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps):
    88   1388.5 MiB      0.0 MiB       return tf.estimator.EvalSpec(
    89   1388.5 MiB      0.0 MiB           input_fn=lambda: _get_dataset(data_path=VAL_DATA,
    90                                                                   BATCH_SIZE=BATCH_SIZE,
    91                                                                   IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST),
    92   1388.5 MiB      0.0 MiB           steps=steps,
    93   1388.5 MiB      0.0 MiB           hooks=None)


Filename: /opt/tf_issue_32052/dummy_datasets.py

Line #    Mem usage    Increment   Line Contents
================================================
   171   1388.5 MiB   1388.5 MiB   @profile
   172                             def _get_dataset(data_path,
   173                                              BATCH_SIZE,
   174                                              IS_EAST_IMAGE_TEST):
   175                                 """
   176                                 Reads TFRecords, decode and batches them
   177                                 :return: dataset
   178                                 """
   179   1388.5 MiB      0.0 MiB       _num_cores = 4
   180   1388.5 MiB      0.0 MiB       _batch_size = BATCH_SIZE
   181                             
   182   1388.5 MiB      0.0 MiB       path = os.path.join(data_path, "*.tfrecords")
   183   1388.5 MiB      0.0 MiB       path = path.replace("//", "/")
   184   1388.5 MiB      0.0 MiB       files = tf.data.Dataset.list_files(path)
   185                                 # files = glob.glob(pathname=path)
   186                             
   187                                 # TF dataset APIs
   188   1388.5 MiB      0.0 MiB       dataset = files.interleave(
   189   1388.5 MiB      0.0 MiB           tf.data.TFRecordDataset,
   190   1388.5 MiB      0.0 MiB           cycle_length=_num_cores,
   191   1388.5 MiB      0.0 MiB           num_parallel_calls=tf.data.experimental.AUTOTUNE)
   192                             
   193                                 # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)
   194                                 # dataset = dataset.shuffle(_batch_size*10, 42)
   195                                 # Map the generator output as features as a dict and label
   196                             
   197   1388.5 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   198                                   dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   199                                 else:
   200   1388.5 MiB      0.0 MiB         dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   201                             
   202   1388.5 MiB      0.0 MiB       dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)
   203                                 # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
   204   1388.5 MiB      0.0 MiB       iterator = dataset.make_one_shot_iterator()
   205   1388.5 MiB      0.0 MiB       batch_feats, batch_label = iterator.get_next()
   206   1388.5 MiB      0.0 MiB       return batch_feats, batch_label


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    18   1388.5 MiB   1388.5 MiB       @profile
    19                                 def _get_optimizer(self, loss):
    20   1388.5 MiB      0.0 MiB           with tf.name_scope("optimizer") as scope:
    21                             
    22   1388.5 MiB      0.0 MiB               global_step = tf.compat.v1.train.get_global_step()
    23   1388.5 MiB      0.0 MiB               learning_rate = tf.compat.v1.train.exponential_decay(0.001,
    24   1388.5 MiB      0.0 MiB                                                                    global_step,
    25   1388.5 MiB      0.0 MiB                                                                    decay_steps=100,
    26   1388.5 MiB      0.0 MiB                                                                    decay_rate=0.94,
    27   1388.5 MiB      0.0 MiB                                                                    staircase=True)
    28                             
    29   1388.5 MiB      0.0 MiB               optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
    30   1388.5 MiB      0.0 MiB                                                    beta_1=0.9,
    31   1388.5 MiB      0.0 MiB                                                    beta_2=0.999,
    32   1388.5 MiB      0.0 MiB                                                    epsilon=1e-7,
    33   1388.5 MiB      0.0 MiB                                                    amsgrad=False,
    34   1388.5 MiB      0.0 MiB                                                    name='Adam')
    35                             
    36   1388.5 MiB      0.0 MiB               optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    37                             
    38                                         # Get both the unconditional updates (the None part)
    39                                         # and the input-conditional updates (the features part).
    40                                         # update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    41                                         # Compute the minimize_op.
    42   1388.5 MiB      0.0 MiB               minimize_op = optimizer.get_updates(
    43   1388.5 MiB      0.0 MiB                   loss,
    44   1388.5 MiB      0.0 MiB                   tf.compat.v1.trainable_variables())[0]
    45   1388.5 MiB      0.0 MiB               train_op = tf.group(minimize_op)
    46   1388.5 MiB      0.0 MiB               return train_op


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1388.5 MiB   1388.5 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1388.5 MiB      0.0 MiB           features = features['data']
    52                             
    53   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1388.5 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1388.5 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1388.5 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1388.5 MiB      0.0 MiB           loss = None
    63   1388.5 MiB      0.0 MiB           optimizer = None
    64   1388.5 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1388.5 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67   1388.5 MiB      0.0 MiB               mse = tf.keras.losses.MeanSquaredError()
    68   1388.5 MiB      0.0 MiB               loss = mse(logits, label)
    69   1388.5 MiB      0.0 MiB               tf.summary.scalar('total_loss', loss)
    70                             
    71   1388.5 MiB      0.0 MiB               optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1388.5 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
I0903 11:08:50.151258 140674526709568 estimator.py:1147] Done calling model_fn.
I0903 11:08:50.333263 140674526709568 evaluation.py:255] Starting evaluation at 2019-09-03T11:08:50Z
I0903 11:08:51.480299 140674526709568 monitored_session.py:240] Graph was finalized.
2019-09-03 11:08:51.481191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:51.481608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:08:51.481647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:08:51.481660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:08:51.481671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:08:51.481683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:08:51.481694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:08:51.481705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:08:51.481716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:08:51.481768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:51.482182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:51.482535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:08:51.482553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:08:51.482558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:08:51.482562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:08:51.482731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:51.483132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:08:51.483484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:08:51.484409 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-8442
I0903 11:08:51.589699 140674526709568 session_manager.py:500] Running local_init_op.
I0903 11:08:51.605019 140674526709568 session_manager.py:502] Done running local_init_op.
I0903 11:08:52.403125 140674526709568 evaluation.py:275] Finished evaluation at 2019-09-03-11:08:52
I0903 11:08:52.403845 140674526709568 estimator.py:2039] Saving dict for global step 8442: global_step = 8442, loss = 36906.95
I0903 11:08:52.404969 140674526709568 estimator.py:2099] Saving 'checkpoint_path' summary for global step 8442: /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-8442
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:44<00:00, 33.37s/it]
E0903 11:08:58.708374 140674526709568 print_helper.py:68] [31m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch[0m
I0903 11:08:58.718701 140674526709568 tf_memory_test.py:135] Saving model to =======> /opt/tf_issue_32052/data/fwd_nnet/export
I0903 11:08:58.773225 140674526709568 estimator.py:1145] Calling model_fn.
I0903 11:09:00.762541 140674526709568 estimator.py:1147] Done calling model_fn.
W0903 11:09:00.763207 140674526709568 deprecation.py:323] From /home/mageswarand/.conda/envs/default/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
I0903 11:09:00.764544 140674526709568 export_utils.py:170] Signatures INCLUDED in export for Classify: None
I0903 11:09:00.764915 140674526709568 export_utils.py:170] Signatures INCLUDED in export for Regress: None
I0903 11:09:00.765274 140674526709568 export_utils.py:170] Signatures INCLUDED in export for Predict: ['predict', 'serving_default']
I0903 11:09:00.765626 140674526709568 export_utils.py:170] Signatures INCLUDED in export for Train: None
I0903 11:09:00.765978 140674526709568 export_utils.py:170] Signatures INCLUDED in export for Eval: None
2019-09-03 11:09:00.766734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:09:00.767225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:01:00.0
2019-09-03 11:09:00.767270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-03 11:09:00.767291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-03 11:09:00.767310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-03 11:09:00.767329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-03 11:09:00.767350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-03 11:09:00.767369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-03 11:09:00.767388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-03 11:09:00.767450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:09:00.767891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:09:00.768254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-03 11:09:00.768276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-03 11:09:00.768284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-03 11:09:00.768290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-03 11:09:00.768470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:09:00.768880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-03 11:09:00.769244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
I0903 11:09:00.982151 140674526709568 saver.py:1284] Restoring parameters from /opt/tf_issue_32052/data/fwd_nnet/model.ckpt-8442
I0903 11:09:01.008104 140674526709568 builder_impl.py:662] Assets added to graph.
I0903 11:09:01.008520 140674526709568 builder_impl.py:457] No assets to write.
I0903 11:09:01.117486 140674526709568 builder_impl.py:422] SavedModel written to: /opt/tf_issue_32052/data/fwd_nnet/export/temp-b'1567489138'/saved_model.pb
    74   1388.5 MiB      0.0 MiB               mode=mode,
    75   1388.5 MiB      0.0 MiB               predictions=predictions,
    76   1388.5 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1388.5 MiB      0.0 MiB               loss=loss,
    78   1388.5 MiB      0.0 MiB               train_op=optimizer,
    79   1388.5 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   108   1388.5 MiB   1388.5 MiB   @profile
   109                             def evaluate(estimator, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, steps=None, checkpoint_path=None):
   110   1388.5 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA, steps=steps,
   111   1388.5 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,
   112   1388.5 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   113   1388.5 MiB      0.0 MiB       estimator.evaluate(
   114   1388.5 MiB      0.0 MiB           input_fn=eval_spec.input_fn,
   115   1388.5 MiB      0.0 MiB           steps=eval_spec.steps,
   116   1388.5 MiB      0.0 MiB           hooks=eval_spec.hooks,
   117   1389.2 MiB      0.7 MiB           checkpoint_path=checkpoint_path)


objgraph growth list start
objgraph growth list end
Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   120   1392.1 MiB   1392.1 MiB   @profile
   121                             def serving_input_receiver_fn(IS_EAST_MODEL):
   122   1392.1 MiB      0.0 MiB       if IS_EAST_MODEL:
   123                                     inputs = {
   124                                         "images": tf.compat.v1.placeholder(tf.float32, [None, None, None, 3]),
   125                                     }
   126                                 else:
   127                                     inputs = {
   128   1392.1 MiB      0.0 MiB               "data": tf.compat.v1.placeholder(tf.float32, [None, 1, 250]),
   129                                     }
   130   1392.1 MiB      0.0 MiB       return tf.estimator.export.ServingInputReceiver(inputs, inputs)


Filename: /opt/tf_issue_32052/simple_ffwd_net.py

Line #    Mem usage    Increment   Line Contents
================================================
    48   1392.1 MiB   1392.1 MiB       @profile
    49                                 def _build(self, features, label, params, mode, config=None):
    50                             
    51   1392.1 MiB      0.0 MiB           features = features['data']
    52                             
    53   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(1024, activation='relu')(features)
    54   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(512, activation='relu')(net)
    55   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(256, activation='relu')(net)
    56   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(128, activation='relu')(net)
    57   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(64, activation='relu')(net)
    58   1392.1 MiB      0.0 MiB           net = tf.keras.layers.Dense(32, activation='relu')(net)
    59   1392.1 MiB      0.0 MiB           logits = tf.keras.layers.Dense(2, activation='softmax')(net)
    60   1392.1 MiB      0.0 MiB           classes = tf.math.greater(logits, 0.5)
    61                             
    62   1392.1 MiB      0.0 MiB           loss = None
    63   1392.1 MiB      0.0 MiB           optimizer = None
    64   1392.1 MiB      0.0 MiB           predictions = {"probability" : logits, "classes" : classes}
    65                             
    66   1392.1 MiB      0.0 MiB           if mode != tf.estimator.ModeKeys.PREDICT:
    67                                         mse = tf.keras.losses.MeanSquaredError()
    68                                         loss = mse(logits, label)
    69                                         tf.summary.scalar('total_loss', loss)
    70                             
    71                                         optimizer = self._get_optimizer(loss=loss)
    72                             
    73   1392.1 MiB      0.0 MiB           return tf.estimator.EstimatorSpec(
    74   1392.1 MiB      0.0 MiB               mode=mode,
    75   1392.1 MiB      0.0 MiB               predictions=predictions,
    76   1392.1 MiB      0.0 MiB               export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)},
    77   1392.1 MiB      0.0 MiB               loss=loss,
    78   1392.1 MiB      0.0 MiB               train_op=optimizer,
    79   1392.1 MiB      0.0 MiB               eval_metric_ops=None)


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   133   1391.7 MiB   1391.7 MiB   @profile
   134                             def export_model(estimator, model_export_path, IS_EAST_MODEL):
   135   1391.7 MiB      0.0 MiB       logging.info("Saving model to =======> {}".format(model_export_path))
   136   1391.7 MiB      0.0 MiB       if not os.path.exists(model_export_path):
   137                                     os.makedirs(model_export_path)
   138   1391.7 MiB      0.0 MiB       estimator.export_saved_model(
   139   1391.7 MiB      0.0 MiB           model_export_path,
   140   1392.2 MiB      0.4 MiB           serving_input_receiver_fn=lambda : serving_input_receiver_fn(IS_EAST_MODEL=IS_EAST_MODEL))


Filename: tf_memory_test.py

Line #    Mem usage    Increment   Line Contents
================================================
   166    303.2 MiB    303.2 MiB   @profile
   167                             def main(args):
   168                             
   169    303.2 MiB      0.0 MiB       memory_used = []
   170    303.2 MiB      0.0 MiB       process = psutil.Process(os.getpid())
   171                             
   172                                 #TODO add into argparser
   173    303.2 MiB      0.0 MiB       IS_EAST_IMAGE_TEST = True
   174                             
   175    303.2 MiB      0.0 MiB       NUM_ARRAYS_PER_FILE = 10000
   176                             
   177                                 #TODO decode function needs this value as part of dataset map function,  hence for now harcoded value
   178                                 # if needed chnage manually at func `numpy_array_decode` in dummy_dataset.py also
   179    303.2 MiB      0.0 MiB       NUM_FEATURES = 250
   180                             
   181    303.2 MiB      0.0 MiB       NUM_IMAGES_PER_FILE = 8
   182                             
   183    303.2 MiB      0.0 MiB       BATCH_SIZE = 4
   184    303.2 MiB      0.0 MiB       TRAIN_DATA = os.getcwd() + "/data/train_data_img"
   185    303.2 MiB      0.0 MiB       VAL_DATA = os.getcwd() + "/data/val_data_img"
   186    303.2 MiB      0.0 MiB       MODEL_DIR = os.getcwd() + "/data/" + "east_net"
   187    303.2 MiB      0.0 MiB       EXPORT_DIR = MODEL_DIR + "/" + "export"
   188    303.2 MiB      0.0 MiB       NUM_EPOCHS = 3
   189    303.2 MiB      0.0 MiB       NUM_SAMPLES_PER_FILE = NUM_IMAGES_PER_FILE
   190                             
   191                             
   192    303.2 MiB      0.0 MiB       if args["dataset"] == "numpy":
   193    303.2 MiB      0.0 MiB           IS_EAST_IMAGE_TEST = False
   194    303.2 MiB      0.0 MiB           BATCH_SIZE = 128
   195    303.2 MiB      0.0 MiB           TRAIN_DATA = os.getcwd() + "/data/train_data"
   196    303.2 MiB      0.0 MiB           VAL_DATA = os.getcwd() + "/data/val_data"
   197    303.2 MiB      0.0 MiB           MODEL_DIR = os.getcwd() + "/" + "data/fwd_nnet"
   198    303.2 MiB      0.0 MiB           EXPORT_DIR = MODEL_DIR + "/" + "export"
   199    303.2 MiB      0.0 MiB           NUM_EPOCHS = 5
   200    303.2 MiB      0.0 MiB           NUM_SAMPLES_PER_FILE = NUM_ARRAYS_PER_FILE
   201                                 elif args["dataset"] == "east":
   202                                     pass
   203                                 else:
   204                                     print_error("Invalid dataset")
   205                             
   206    303.2 MiB      0.0 MiB       TOTAL_STEPS_PER_FILE = NUM_SAMPLES_PER_FILE / BATCH_SIZE
   207                             
   208    303.2 MiB      0.0 MiB       if args["delete"] == True:
   209                                     print_info("Deleting old data files")
   210                                     shutil.rmtree(TRAIN_DATA)
   211                                     shutil.rmtree(VAL_DATA)
   212                             
   213    303.2 MiB      0.0 MiB       gen_data(IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST,
   214    303.2 MiB      0.0 MiB                TRAIN_DATA=TRAIN_DATA,
   215    303.2 MiB      0.0 MiB                VAL_DATA=VAL_DATA,
   216    303.2 MiB      0.0 MiB                NUM_SAMPLES_PER_FILE=NUM_SAMPLES_PER_FILE,
   217    303.2 MiB      0.0 MiB                NUM_FEATURES=NUM_FEATURES,
   218    303.7 MiB      0.5 MiB                number_files=int(args["num_tfrecord_files"]))
   219                             
   220    303.7 MiB      0.0 MiB       if args["mode"] == "test_iterator":
   221                                     print('objgraph growth list start')
   222                                     objgraph.show_growth(limit=50)
   223                                     print('objgraph growth list end')
   224                             
   225                             
   226                                     test_dataset(data_path=TRAIN_DATA,
   227                                                  BATCH_SIZE=BATCH_SIZE,
   228                                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   229                                     # test_dataset(VAL_DATA)
   230                                     print('objgraph growth list start')
   231                                     objgraph.show_growth(limit=50)
   232                                     print('objgraph growth list end')
   233                             
   234                                     return
   235                             
   236                                 # print(dataset_to_iterator(data_path=TRAIN_DATA))
   237                             
   238    303.7 MiB      0.0 MiB       if IS_EAST_IMAGE_TEST:
   239                                     model = EASTTFModel(model_root_directory="store")
   240                                 else:
   241    303.7 MiB      0.0 MiB           model = NNet()
   242                             
   243    303.7 MiB      0.0 MiB       estimator = tf.estimator.Estimator(model_fn=model,
   244    303.7 MiB      0.0 MiB                                          config=_init_tf_config(TOTAL_STEPS_PER_FILE=TOTAL_STEPS_PER_FILE,
   245    303.7 MiB      0.0 MiB                                                                 MODEL_DIR=MODEL_DIR), params=None)
   246    303.7 MiB      0.0 MiB       memory_usage_psutil()
   247    303.7 MiB      0.0 MiB       print('objgraph growth list start')
   248    305.5 MiB      1.8 MiB       objgraph.show_growth(limit=50)
   249    305.5 MiB      0.0 MiB       print('objgraph growth list end')
   250                             
   251                                 # print(objgraph.get_leaking_objects())
   252                             
   253   1388.5 MiB      0.0 MiB       for epoch in tqdm(range(NUM_EPOCHS)):
   254                             
   255   1388.1 MiB      0.0 MiB           print("\n\n\n\n\n\n")
   256   1388.1 MiB      0.0 MiB           print_error(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch")
   257   1388.1 MiB      0.0 MiB           memory_usage_psutil()
   258   1388.1 MiB      0.0 MiB           memory_used.append(process.memory_info()[0] / float(2 ** 20))
   259   1388.1 MiB      0.0 MiB           print_error(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Training")
   260   1388.1 MiB      0.0 MiB           train(estimator=estimator,
   261   1388.1 MiB      0.0 MiB                 TRAIN_DATA=TRAIN_DATA,
   262   1388.1 MiB      0.0 MiB                 BATCH_SIZE=BATCH_SIZE,
   263   1388.5 MiB   1001.8 MiB                 IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   264   1388.5 MiB      0.0 MiB           print_error(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Evaluating")
   265   1388.5 MiB      0.0 MiB           evaluate(estimator=estimator,
   266   1388.5 MiB      0.0 MiB                    VAL_DATA=VAL_DATA,
   267   1388.5 MiB      0.0 MiB                    BATCH_SIZE=BATCH_SIZE,
   268   1389.2 MiB      7.6 MiB                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)
   269   1389.2 MiB      0.0 MiB           print('objgraph growth list start')
   270   1388.5 MiB      2.0 MiB           objgraph.show_growth(limit=50)
   271   1388.5 MiB      0.0 MiB           print('objgraph growth list end')
   272                             
   273                             
   274   1391.2 MiB      2.8 MiB       plt.plot(memory_used)
   275   1391.2 MiB      0.0 MiB       plt.title('Evolution of memory')
   276   1391.2 MiB      0.0 MiB       plt.xlabel('iteration')
   277   1391.2 MiB      0.0 MiB       plt.ylabel('memory used (MB)')
   278   1391.7 MiB      0.5 MiB       plt.savefig("logs/" + args["dataset"] + "_memory_usage.png")
   279   1391.7 MiB      0.0 MiB       plt.show()
   280                             
   281   1391.7 MiB      0.0 MiB       print_error(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New Epoch")
   282   1392.2 MiB      0.4 MiB       export_model(estimator=estimator, model_export_path=EXPORT_DIR, IS_EAST_MODEL=IS_EAST_IMAGE_TEST)
   283                             
   284   1404.4 MiB     12.2 MiB       (objgraph.get_leaking_objects())


Top 100 lines
#1: python3.7/linecache.py:137: 2275.7 KiB
    lines = fp.readlines()
#2: python3.7/posixpath.py:365: 502.4 KiB
    path = sep*initial_slashes + path
#3: site-packages/memory_profiler.py:710: 480.8 KiB
    self._original_trace_function(frame, event, arg)
#4: python3.7/abc.py:143: 321.1 KiB
    return _abc_subclasscheck(cls, subclass)
#5: python3.7/inspect.py:742: 144.0 KiB
    os.path.realpath(f)] = module.__name__
#6: python3.7/inspect.py:738: 144.0 KiB
    _filesbymodname[modname] = f
#7: site-packages/memory_profiler.py:694: 119.9 KiB
    if frame.f_code in self.code_map:
#8: framework/ops.py:5483: 77.0 KiB
    yield g
#9: python3.7/ast.py:326: 74.3 KiB
    new_node = self.visit(old_value)
#10: <frozen importlib._bootstrap_external>:525: 58.1 KiB
#11: python3.7/contextlib.py:82: 55.0 KiB
    self.gen = func(*args, **kwds)
#12: core/converter.py:346: 44.1 KiB
    return super(Base, self).visit(node)
#13: python3.7/ast.py:262: 39.6 KiB
    return visitor(node)
#14: <string>:1: 39.3 KiB
#15: site-packages/objgraph.py:309: 36.0 KiB
    peak_stats[name] = count
#16: static_analysis/reaching_definitions.py:307: 32.4 KiB
    node = super(TreeAnnotator, self).visit(node)
#17: static_analysis/liveness.py:161: 23.9 KiB
    node = super(Annotator, self).visit(node)
#18: python3.7/inspect.py:2215: 19.0 KiB
    sigcls=sigcls)
#19: python3.7/ast.py:266: 18.8 KiB
    for field, value in iter_fields(node):
#20: tensorboard/lazy.py:54: 18.1 KiB
    self.__dict__.update(module.__dict__)
#21: psutil/_pslinux.py:1724: 15.9 KiB
    ctime = float(self._parse_stat_file()['create_time'])
#22: python3.7/sre_parse.py:426: 14.1 KiB
    not nested and not items))
#23: protobuf/text_format.py:455: 13.3 KiB
    self.PrintMessage(value)
#24: framework/op_def_library.py:527: 12.2 KiB
    preferred_dtype=default_dtype)
#25: framework/op_def_library.py:54: 11.3 KiB
    allowed_list = attr_def.allowed_values.list.type
#26: ops/gen_resource_variable_ops.py:587: 11.2 KiB
    "ReadVariableOp", resource=resource, dtype=dtype, name=name)
#27: site-packages/objgraph.py:311: 11.1 KiB
    reverse=True)
#28: pyct/transformer.py:371: 10.6 KiB
    replacement = self.visit(node)
#29: ops/gen_array_ops.py:6304: 10.3 KiB
    "Pack", values=values, axis=axis, name=name)
#30: python3.7/sre_compile.py:783: 9.8 KiB
    groupindex, tuple(indexgroup)
#31: ops/variables.py:2743: 9.4 KiB
    new_op = _safe_initial_value_from_op(name, op, op_cache)
#32: site-packages/memory_profiler.py:781: 9.4 KiB
    stream.write(unicode(tmp, 'UTF-8'))
#33: pyct/origin_info.py:245: 8.5 KiB
    source_lines = source.split('\n')
#34: util/tf_should_use.py:134: 8.3 KiB
    copy_tx = type(tx.__name__, tx.__bases__, dict(tx.__dict__))
#35: python3.7/tempfile.py:550: 7.7 KiB
    newline=newline, encoding=encoding)
#36: python3.7/threading.py:348: 7.7 KiB
    waiters_to_notify = _deque(_islice(all_waiters, n))
#37: astor/node_util.py:143: 7.5 KiB
    return visitor(node)
#38: matplotlib/transforms.py:159: 7.3 KiB
    value=value, invalidating_node=self)
#39: framework/ops.py:1365: 7.2 KiB
    ctx=ctx))
#40: tensorflow/__init__.py:46: 7.0 KiB
    self.__dict__.update(module.__dict__)
#41: framework/ops.py:5631: 7.0 KiB
    yield
#42: pyct/origin_info.py:138: 6.9 KiB
    source_map[line_loc] = origin_info
#43: framework/op_def_library.py:378: 6.9 KiB
    deprecation_version = op_def.deprecation.version
#44: python3.7/threading.py:238: 6.6 KiB
    self._waiters = _deque()
#45: protobuf/text_format.py:192: 6.5 KiB
    field.message_type.GetOptions().map_entry)
#46: framework/func_graph.py:410: 6.0 KiB
    self._auto_cast_variable_read_dtype = old_auto_cast_var_read_dtype
#47: matplotlib/artist.py:63: 5.6 KiB
    self.axes.stale = val
#48: matplotlib/text.py:429: 5.4 KiB
    ret = bbox, list(zip(lines, whs, xs, ys)), descent
#49: python3.7/ast.py:317: 5.3 KiB
    value = self.visit(value)
#50: site-packages/objgraph.py:1109: 5.1 KiB
    return _get_obj_type(obj).__name__
#51: framework/function.py:851: 5.1 KiB
    op = self._add_op_and_parents(tensor.op)
#52: python3.7/ast.py:35: 5.0 KiB
    return compile(source, filename, mode, PyCF_ONLY_AST)
#53: matplotlib/transforms.py:94: 4.7 KiB
    self._parents = {}
#54: matplotlib/font_manager.py:1291: 4.5 KiB
    return _get_font(filename, hinting_factor)
#55: impl/conversion.py:621: 4.5 KiB
    ag_internal.__dict__.update(operators.__dict__)
#56: axes/_base.py:1072: 4.5 KiB
    horizontalalignment='right',
#57: <frozen importlib._bootstrap_external>:59: 4.4 KiB
#58: pyct/transformer.py:237: 4.3 KiB
    self.state = _State()
#59: ops/gradients_util.py:679: 4.3 KiB
    lambda: grad_fn(op, *out_grads))
#60: framework/func_graph.py:195: 4.3 KiB
    self.control_outputs = []
#61: matplotlib/patches.py:423: 4.3 KiB
    self._capstyle = s
#62: matplotlib/axis.py:168: 4.3 KiB
    if grid_alpha is None else grid_alpha)
#63: python3.7/ast.py:312: 4.3 KiB
    for field, old_value in iter_fields(node):
#64: matplotlib/transforms.py:2398: 4.1 KiB
    invalidating_node=invalidating_node)
#65: pyct/qual_names.py:97: 4.0 KiB
    self.qn = (base,)
#66: matplotlib/transforms.py:827: 3.9 KiB
    points = np.array(args, dtype=float).reshape(2, 2)
#67: matplotlib/figure.py:57: 3.9 KiB
    self.figure.stale = val
#68: matplotlib/artist.py:50: 3.7 KiB
    return draw(artist, renderer, *args, **kwargs)
#69: pyct/anno.py:123: 3.5 KiB
    node._fields += (field_name,)
#70: framework/op_def_library.py:397: 3.5 KiB
    for attr_def in op_def.attr:
#71: backends/backend_agg.py:149: 3.3 KiB
    self._renderer.draw_path(gc, path, transform, rgbFace)
#72: site-packages/objgraph.py:315: 3.3 KiB
    return [(name, stats[name], delta) for name, delta in deltas]
#73: python3.7/sre_compile.py:148: 3.3 KiB
    _compile(code, av[2], flags)
#74: eager/function.py:2038: 3.3 KiB
    capture_by_value=self._capture_by_value),
#75: ops/math_grad.py:1580: 3.2 KiB
    t_a = op.get_attr("transpose_a")
#76: gast/astn.py:17: 3.1 KiB
    def generic_visit(self, node):
#77: matplotlib/transforms.py:2461: 3.1 KiB
    return Affine2D(np.dot(self._b.get_affine().get_matrix(),
#78: util/dispatch.py:180: 3.0 KiB
    return target(*args, **kwargs)
#79: framework/op_def_library.py:793: 2.9 KiB
    op_def=op_def)
#80: util/deprecation.py:507: 2.9 KiB
    return func(*args, **kwargs)
#81: tkinter/__init__.py:1705: 2.7 KiB
    return self.func(*args)
#82: python3.7/threading.py:552: 2.7 KiB
    signaled = self._cond.wait(timeout)
#83: ops/gen_math_ops.py:11176: 2.7 KiB
    _attrs = ("keep_dims", _op.get_attr("keep_dims"), "T",
#84: static_analysis/activity.py:436: 2.7 KiB
    node.args = self.visit(node.args)
#85: site-packages/memory_profiler.py:660: 2.6 KiB
    return func(*args, **kwds)
#86: matplotlib/transforms.py:781: 2.6 KiB
    self._points_orig = self._points.copy()
#87: python3.7/inspect.py:732: 2.6 KiB
    for modname, module in list(sys.modules.items()):
#88: python3.7/weakref.py:407: 2.6 KiB
    self.data[ref(key, self._remove)] = value
#89: pyct/origin_info.py:181: 2.5 KiB
    return node.lineno + self._lineno_offset
#90: python3.7/tempfile.py:146: 2.5 KiB
    self._rng = _Random()
#91: framework/ops.py:4228: 2.5 KiB
    yield
#92: matplotlib/axis.py:1190: 2.5 KiB
    tick.draw(renderer)
#93: tmp/tmphza0pvyl.py:48: 2.4 KiB
    cond = ag__.converted_call(filename.endswith, add_scope.callopts, (('.pyc', '.pyo'),), None, add_scope)
#94: operators/control_flow.py:1004: 2.3 KiB
    return body() if cond else orelse()
#95: core/_internal.py:282: 2.3 KiB
    c_arr = (ctypes.c_char * 0).from_buffer(simple_arr)
#96: matplotlib/font_manager.py:1229: 2.3 KiB
    self.score_stretch(prop.get_stretch(), font.stretch) + \
#97: ops/cond_v2.py:553: 2.3 KiB
    def _make_output_composite_tensors_match(op_type, branch_graphs):
#98: util/module_wrapper.py:93: 2.2 KiB
    self.__dict__.update(wrapped.__dict__)
#99: tqdm/_tqdm.py:516: 2.2 KiB
    cls.monitor = None
#100: estimator/__init__.py:40: 2.2 KiB
    from tensorflow_estimator.python.estimator.exporter import BestExporter
1793 other: 860.3 KiB
1793 other: 0.8 MiB
Total allocated size: 5811.3 KiB
Total allocated size: 5.7 MiB
